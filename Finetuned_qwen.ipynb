{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9531edd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\LLM_prod\\Genomic-QA-Fine-Tuning\\LLM\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 5/5 [00:16<00:00,  3.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 637,665,280 || all params: 8,828,400,640 || trainable%: 7.2229\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fadial\\AppData\\Local\\Temp\\ipykernel_26348\\524094314.py:203: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Training samples: 4800\n",
      "Evaluation samples: 200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "d:\\LLM_prod\\Genomic-QA-Fine-Tuning\\LLM\\lib\\site-packages\\torch\\utils\\checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "d:\\LLM_prod\\Genomic-QA-Fine-Tuning\\LLM\\lib\\site-packages\\transformers\\integrations\\sdpa_attention.py:54: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:263.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='350' max='3000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 350/3000 2:32:54 < 19:24:22, 0.04 it/s, Epoch 2/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.252300</td>\n",
       "      <td>1.488135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.091900</td>\n",
       "      <td>0.077413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.070700</td>\n",
       "      <td>0.073298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.069000</td>\n",
       "      <td>0.070860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.069400</td>\n",
       "      <td>0.070087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.069300</td>\n",
       "      <td>0.069944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.067100</td>\n",
       "      <td>0.070104</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\LLM_prod\\Genomic-QA-Fine-Tuning\\LLM\\lib\\site-packages\\peft\\utils\\save_and_load.py:195: UserWarning: Could not find a config file in C:\\Users\\fadial\\Downloads\\qwen - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "d:\\LLM_prod\\Genomic-QA-Fine-Tuning\\LLM\\lib\\site-packages\\torch\\utils\\checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "d:\\LLM_prod\\Genomic-QA-Fine-Tuning\\LLM\\lib\\site-packages\\peft\\utils\\save_and_load.py:195: UserWarning: Could not find a config file in C:\\Users\\fadial\\Downloads\\qwen - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "d:\\LLM_prod\\Genomic-QA-Fine-Tuning\\LLM\\lib\\site-packages\\torch\\utils\\checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "d:\\LLM_prod\\Genomic-QA-Fine-Tuning\\LLM\\lib\\site-packages\\peft\\utils\\save_and_load.py:195: UserWarning: Could not find a config file in C:\\Users\\fadial\\Downloads\\qwen - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "d:\\LLM_prod\\Genomic-QA-Fine-Tuning\\LLM\\lib\\site-packages\\torch\\utils\\checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\LLM_prod\\Genomic-QA-Fine-Tuning\\LLM\\lib\\site-packages\\peft\\utils\\save_and_load.py:195: UserWarning: Could not find a config file in C:\\Users\\fadial\\Downloads\\qwen - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model response:\n",
      "### Human: What is the equation used for attention scores in transformer architectures?\n",
      "### Assistant: Attention(Q,K,V) = softmax(QK^T/sqrt(d_k))V, where Q, K, and V are query, key, and value matrices. The attention scores are the softmax scores in the attention matrix, which is QK^T/sqrt(d_k). These scores are then used to weight the values matrix V to produce the final output. The denominator sqrt(d_k) is a scaling factor to prevent large values in the dot product, and d_k is the dimensionality\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    EarlyStoppingCallback,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "from datasets import load_dataset\n",
    "from peft import get_peft_model, LoraConfig, TaskType, prepare_model_for_kbit_training\n",
    "import os\n",
    "from transformers import TrainerCallback\n",
    "import numpy as np\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "# 0. Configuration - Optimized for Colab\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "LOCAL_MODEL_PATH = r\"C:\\Users\\fadial\\Downloads\\qwen\"\n",
    "DATASET_NAME = \"genomic_qa_dataset_5000.json\"  # Your Q&A dataset\n",
    "SAMPLE_SIZE = 4800  # Training samples\n",
    "EVAL_SIZE = 200    # Evaluation samples\n",
    "MAX_LENGTH = 256  # Optimal for instruction-response\n",
    "\n",
    "# Configure 4-bit quantization\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True\n",
    ")\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "# 1. Model Loading\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    LOCAL_MODEL_PATH,\n",
    "    local_files_only=True,\n",
    "    trust_remote_code=True,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    offload_folder=\"offload\",\n",
    "    offload_state_dict=True\n",
    ")\n",
    "\n",
    "# Prepare model for k-bit training\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "# 2. Tokenizer Setup - MUST BE BEFORE DATASET PROCESSING\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    LOCAL_MODEL_PATH,\n",
    "    local_files_only=True,\n",
    "    trust_remote_code=True,\n",
    "    use_fast=False\n",
    ")\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "# 3. LoRA Configuration\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    inference_mode=False,\n",
    "    r=16,  # Increased from 8 → better task adaptation\n",
    "    lora_alpha=32,  # Increased from 16 → balances scale vs. regularization\n",
    "    lora_dropout=0.05,  # Lowered from 0.1 → reduces over-regularization\n",
    "    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"],  # Added modules\n",
    "    bias=\"lora_only\",  # Changed from \"none\" → better fine-tuning\n",
    "    modules_to_save=[\"lm_head\"],  # Crucial addition → improves output quality\n",
    "    use_rslora=True  # New → better stability & convergence\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "# 4. Dataset Preparation & Formatting\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "def format_qa(examples):\n",
    "    \"\"\"Format question-answer pairs into instruction-response format\"\"\"\n",
    "    texts = []\n",
    "    for q, a in zip(examples[\"question\"], examples[\"answer\"]):\n",
    "        texts.append(f\"### Human: {q}\\n### Assistant: {a}\")\n",
    "    return {\"text\": texts}\n",
    "\n",
    "# Load dataset\n",
    "dataset = load_dataset(\"json\", data_files=DATASET_NAME)\n",
    "\n",
    "# Format into instruction-response pairs\n",
    "dataset = dataset.map(\n",
    "    format_qa,\n",
    "    batched=True,\n",
    "    remove_columns=[\"question\", \"answer\"]\n",
    ")\n",
    "\n",
    "# Create subsets - fixed to avoid negative indexing\n",
    "train_data = dataset[\"train\"].select(range(SAMPLE_SIZE))\n",
    "eval_data = dataset[\"train\"].select(range(SAMPLE_SIZE, SAMPLE_SIZE+EVAL_SIZE))\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "# 5. Tokenization with Label Masking\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "def tokenize_function(examples):\n",
    "    # Tokenize with padding\n",
    "    tokenized = tokenizer(\n",
    "        examples[\"text\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=MAX_LENGTH,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    # Create labels by copying input_ids\n",
    "    labels = tokenized[\"input_ids\"].clone()\n",
    "    \n",
    "    # Mask instruction tokens\n",
    "    for i in range(labels.shape[0]):\n",
    "        input_ids = tokenized[\"input_ids\"][i]\n",
    "        \n",
    "        # Find \"Assistant:\" token positions\n",
    "        assistant_pos = None\n",
    "        for idx in range(len(input_ids) - 1):\n",
    "            if tokenizer.decode(input_ids[idx:idx+1]) == \"Assistant\":\n",
    "                if tokenizer.decode(input_ids[idx:idx+8]) == \"Assistant:\":\n",
    "                    assistant_pos = idx\n",
    "                    break\n",
    "        \n",
    "        # Mask everything before \"Assistant:\"\n",
    "        if assistant_pos is not None:\n",
    "            labels[i, :assistant_pos+8] = -100\n",
    "    \n",
    "    return {\n",
    "        \"input_ids\": tokenized[\"input_ids\"],\n",
    "        \"attention_mask\": tokenized[\"attention_mask\"],\n",
    "        \"labels\": labels\n",
    "    }\n",
    "\n",
    "# Apply tokenization\n",
    "train_dataset = train_data.map(\n",
    "    tokenize_function, \n",
    "    batched=True, \n",
    "    batch_size=4,\n",
    "    remove_columns=[\"text\"]\n",
    ")\n",
    "\n",
    "eval_dataset = eval_data.map(\n",
    "    tokenize_function, \n",
    "    batched=True, \n",
    "    batch_size=4,\n",
    "    remove_columns=[\"text\"]\n",
    ")\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "# 6. Data Collator\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False\n",
    ")\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "# 7. Training Arguments\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./genomic-assistant-results\",\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=8,\n",
    "    learning_rate=1e-4,\n",
    "    num_train_epochs=20,\n",
    "    logging_steps=20,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=50,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=100,\n",
    "    fp16=True,\n",
    "    warmup_ratio=0.1,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    save_total_limit=1,\n",
    "    remove_unused_columns=False,\n",
    ")\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "# 8. Memory Management Callback\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "class ClearCacheCallback(TrainerCallback):\n",
    "    def on_step_end(self, args, state, control, **kwargs):\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "# 9. Trainer Setup\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    callbacks=[\n",
    "        EarlyStoppingCallback(early_stopping_patience=1),\n",
    "        ClearCacheCallback()\n",
    "    ]\n",
    ")\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "# 10. Start Training\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "if __name__ == \"__main__\":\n",
    "    os.makedirs(\"offload\", exist_ok=True)\n",
    "    print(\"Starting training...\")\n",
    "    print(f\"Training samples: {len(train_dataset)}\")\n",
    "    print(f\"Evaluation samples: {len(eval_dataset)}\")\n",
    "    \n",
    "    # Start training\n",
    "    trainer.train()\n",
    "    \n",
    "    print(\"Training complete!\")\n",
    "    \n",
    "    # Save final model\n",
    "    model.save_pretrained(\"./genomic-assistant\")\n",
    "    tokenizer.save_pretrained(\"./genomic-assistant\")\n",
    "    \n",
    "    # Test with a genomic question\n",
    "    prompt = \"### Human: What is the equation used for attention scores in transformer architectures?\\n### Assistant:\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=100,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9\n",
    "    )\n",
    "    print(\"Model response:\")\n",
    "    print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4e37b7f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model response:\n",
      "### Human: Which dimensionality reduction technique was used in the hepatitis ARIMA paper?\n",
      "### Assistant: This article presents a novel dimensionality reduction technique using the Auto Regressive Moving Average Model and signal processing for the classification of four different types of the Hepatitis Virus. In this approach, the order selection of the ARIMA model is determined by the Ak\n"
     ]
    }
   ],
   "source": [
    "prompt = \"### Human: Which dimensionality reduction technique was used in the hepatitis ARIMA paper?\\n### Assistant:\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "   max_new_tokens=50,  # Strict limit\n",
    "    do_sample=True,\n",
    "    temperature=0.7,  # Lower than default (0.7-0.9)\n",
    "    top_p=0.92,        # Slightly lower than default\n",
    "    repetition_penalty=1.15 \n",
    ")\n",
    "print(\"Model response:\")\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b81d01",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
