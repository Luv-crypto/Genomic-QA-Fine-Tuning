# ─── run identifier ───────────────────────────────────────────
family:          qwen        # model family name
run_name:        run-000
output_root:     models/hf/wip        # base folder for WIP runs

# ─── base model ───────────────────────────────────────────────
base_model:
  repo:          C:\\Users\\fadial\\Downloads\\qwen   # HF id or local folder
  revision:      main

# ─── dataset ───────────────────────────────────────────────────
dataset:
  path:          data/raw/genomic_qa_dataset_5000.json
  sample_size:   4500
  eval_size:     500

# ─── tokenizer ────────────────────────────────────────────────
tokenizer:
  max_length:    256

# ─── quantisation when LOADING base model ─────────────────────
quantization:
  use_4bit:      true     # false → fp16 load
  quant_type:    nf4
  compute_dtype: float16
  double_quant:  true

# ─── LoRA params ──────────────────────────────────────────────
lora:
  r:             16
  alpha:         32
  dropout:       0.3
  target_modules: ["q_proj","v_proj","k_proj","o_proj"]

# ─── training hyper-params ────────────────────────────────────
training:
  epochs:        10        # keep 1 for smoke test
  lr:            1e-4
  batch_size:    4
  grad_accum:    8
  fp16:          true

# ─── local quick-eval ─────────────────────────────────────────
eval:
  path:          data/raw/eval.json
  batch_size:    4
  max_length:    2048

# ─── CI flags ─────────────────────────────────────────────────
ci:
  quantize:      false     # true → Q4_0 after gguf convert
  quant_mode:    q4_k_m
  env:
    LLAMA_CPP_DIR: D:/LLM_prod/Genomic-QA-Fine-Tuning/llama.cpp          # gets filled by promote.ps1
    LLAMA_PY:      D:/LLM_prod/Genomic-QA-Fine-Tuning/weights_onv/Scripts/python.exe     
